# Make predictions on the test set
nnet_predictions <- predict(nnet_model, newdata = test_data)
# Evaluate the performance of the model
nnet_accuracy <- mean(nnet_predictions == test_data$M)
nnet_accuracy
confusionMatrix(nnet_predictions, test_data$M)
confusionMatrix(nnet_predictions, test_data$M)
nnet_predictions_test <- predict(nnet_model, newdata = test_data)
actual_labels <- test_data$M
confusionMatrix(nnet_predictions, actual_labels)
nnet_predictions_test <- predict(nnet_model, newdata = test_data)
actual_labels <- test_data$M
confusionMatrix(nnet_predictions, actual_labels)
conf_mat <- confusionMatrix(nnet_predictions, test_data$M)
set.seed(123)
train_idx <- sample(nrow(normalized_data), nrow(normalized_data)*0.7)
train_data <- normalized_data[train_idx, ]
test_data <- normalized_data[-train_idx, ]
# Fit a neural network model
nnet_model <- nnet(M ~ V + H + S, data = train_data, size = 3)
# Make predictions on the test set
nnet_predictions <- predict(nnet_model, newdata = test_data)
# Evaluate the performance of the model
nnet_accuracy <- mean(nnet_predictions == test_data$M)
nnet_accuracy
conf_mat <- confusionMatrix(nnet_predictions, test_data$M)
set.seed(123)
train_idx <- sample(nrow(normalized_data), nrow(normalized_data)*0.7)
train_data <- normalized_data[train_idx, ]
test_data <- normalized_data[-train_idx, ]
# Fit a neural network model
nnet_model <- nnet(M ~ V + H + S, data = train_data, size = 3)
# Make predictions on the test set
nnet_predictions <- predict(nnet_model, newdata = test_data)
# Evaluate the performance of the model
nnet_accuracy <- mean(nnet_predictions == test_data$M)
nnet_accuracy
conf_mat <- confusionMatrix(nnet_predictions, test_data$M)
dim(nnet_predictions)
dim(test_data$M)
#conf_mat <- confusionMatrix(nnet_predictions, test_data$M)
#conf_mat$table
dim(nnet_predictions)
dim(test_data)
#conf_mat <- confusionMatrix(nnet_predictions, test_data$M)
#conf_mat$table
dim(nnet_predictions)
dim(test_data$M)
#conf_mat <- confusionMatrix(nnet_predictions, test_data$M)
#conf_mat$table
dim(nnet_predictions)
test_actuals = test_data$MS
dim(test_actuals)
#conf_mat <- confusionMatrix(nnet_predictions, test_data$M)
#conf_mat$table
dim(nnet_predictions)
test_actuals = test_data$M
dim(test_actuals)
#conf_mat <- confusionMatrix(nnet_predictions, test_data$M)
#conf_mat$table
dim(nnet_predictions)
test_actuals = test_data$M
dim(test_actuals)
#conf_mat <- confusionMatrix(nnet_predictions, test_data$M)
#conf_mat$table
set.seed(123)
train_idx <- sample(nrow(normalized_data), nrow(normalized_data)*0.7)
train_data <- normalized_data[train_idx, ]
test_data <- normalized_data[-train_idx, ]
# Fit a neural network model
nnet_model <- nnet(M ~ V + H + S, data = train_data, size = 3)
# Make predictions on the test set
nnet_predictions <- predict(nnet_model, newdata = test_data)
# Evaluate the performance of the model
nnet_accuracy <- mean(nnet_predictions == test_data$M)
dim(test_data$M)
nnet_accuracy
set.seed(123)
train_idx <- sample(nrow(normalized_data), nrow(normalized_data)*0.7)
train_data <- normalized_data[train_idx, ]
test_data <- normalized_data[-train_idx, ]
# Fit a neural network model
nnet_model <- nnet(M ~ V + H + S, data = train_data, size = 3)
# Make predictions on the test set
nnet_predictions <- predict(nnet_model, newdata = test_data)
# Evaluate the performance of the model
nnet_accuracy <- mean(nnet_predictions == test_data$M)
nnet_accuracy
# Fit a multinomial logistic regression model
model <- multinom(M ~ V + H + S, data = normalized_data)
summary(model)
# Extract the test set from my_data using the test logical vector
test_data <- normalized_data[-train_idx, ]
# Make predictions on the test set
predictions <- predict(model, newdata = test_data, type = "class")
# Evaluate the performance of the model
accuracy <- mean(predictions == test_data$M)
dim(test_data$M)
accuracy
# Fit a multinomial logistic regression model
model <- multinom(M ~ V + H + S, data = normalized_data)
summary(model)
# Extract the test set from my_data using the test logical vector
test_data <- normalized_data[-train_idx, ]
# Make predictions on the test set
predictions <- predict(model, newdata = test_data, type = "class")
# Evaluate the performance of the model
accuracy <- mean(predictions == test_data$M)
dim(test_data)
accuracy
dim(nnet_predictions)
test_actuals = test_data$V
dim(test_actuals)
#conf_mat <- confusionMatrix(nnet_predictions, test_data$M)
#conf_mat$table
#dim(nnet_predictions)
#test_actuals = test_data$V
#dim(test_actuals)
#conf_mat <- confusionMatrix(nnet_predictions, test_data$M)
#conf_mat$table
head(test_data$M)
#dim(nnet_predictions)
#test_actuals = test_data$V
#dim(test_actuals)
#conf_mat <- confusionMatrix(nnet_predictions, test_data$M)
#conf_mat$table
head(test_data$M,50)
#dim(nnet_predictions)
#test_actuals = test_data$V
#dim(test_actuals)
#conf_mat <- confusionMatrix(nnet_predictions, test_data$M)
#conf_mat$table
head(test_data$M,100)
nnet_confusion <- table(nnet_predictions, test_data$M)
nnet_confusion
nnet_confusion <- table(nnet_predictions, test_data$M)
nnet_confusion
# Create confusion matrix plot
ggplot(nnet_confusion_df, aes(x = actual, y = predicted, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "white", high = "blue") +
theme_minimal() +
labs(x = "Actual", y = "Predicted", fill = "Count")
nnet_confusion <- table(nnet_predictions, test_data$M)
nnet_confusion
# Create confusion matrix plot
ggplot(nnet_confusion, aes(x = actual, y = predicted, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "white", high = "blue") +
theme_minimal() +
labs(x = "Actual", y = "Predicted", fill = "Count")
nnet_confusion <- table(nnet_predictions, test_data$M)
nnet_confusion
nnet_confusion_df <- as.data.frame.matrix(nnet_confusion)
nnet_confusion_df$predicted <- rownames(nnet_confusion_df)
nnet_confusion_df <- tidyr::gather(nnet_confusion_df, actual, value, -predicted)
# Create confusion matrix plot
ggplot(nnet_confusion_df, aes(x = actual, y = predicted, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "white", high = "blue") +
theme_minimal() +
labs(x = "Actual", y = "Predicted", fill = "Count")
nnet_confusion <- table(nnet_predictions, test_data$M)
nnet_confusion
nnet_confusion_df <- as.data.frame.matrix(nnet_confusion)
nnet_confusion_df$predicted <- rownames(nnet_confusion_df)
nnet_confusion_df <- tidyr::gather(nnet_confusion_df, actual, value, -predicted)
# Create confusion matrix plot
ggplot(nnet_confusion_df, aes(x = actual, y = predicted, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "white", high = "blue") +
theme_minimal() +
labs(x = "Actual", y = "Predicted", fill = "Count")
set.seed(123)
train_pct <- 0.7
train_size <- round(nrow(normalized_data) * train_pct)
train_indices <- sample(seq_len(nrow(normalized_data)), size = train_size)
train_data <- normalized_data[train_indices, ]
test_data <- normalized_data[-train_indices, ]
kmeans_result <- kmeans(train_data[, 1:3], centers = 3)
summary(kmeans_result)
centers <- kmeans_result$centers
# Use the cluster centers to predict the clusters of the test data
test_clusters <- apply(test_data[, 1:3], 1, function(x) {
# Calculate the distance from each point to each center
distances <- apply(centers, 1, function(c) sqrt(sum((x - c)^2)))
# Return the index of the closest center
which.min(distances)
})
print(test_clusters)
plot(test_data[,1], test_data[,2], col = test_clusters)
# Create the decision tree
tree <- rpart(M ~ V + H + S, data = normalized_data, method = "class")
# Plot the decision tree
plot(tree)
text(tree)
# Create a new dataset to predict on
new_data <- data.frame(V = c(1, 2, 3),
H = c(4, 5, 6),
S = c(7, 8, 9))
# Make predictions using the decision tree
predictions <- predict(tree, new_data, type = "class")
predictions
dtree_confusion <- table(predictions, test_data$M)
dim(predictions)
dtree_confusion <- table(predictions, test_data$M)
dim(predictions)
dtree_confusion <- table(predictions, test_data$M)
# Create a new dataset to predict on
new_data <- data.frame(V = c(1, 2, 3),
H = c(4, 5, 6),
S = c(7, 8, 9))
# Make predictions using the decision tree
predictions <- predict(tree, new_data, type = "class")
predictions
dim(predictions)
# Create a new dataset to predict on
new_data <- data.frame(V = c(1, 2, 3),
H = c(4, 5, 6),
S = c(7, 8, 9))
# Make predictions using the decision tree
predictions <- predict(tree, new_data, type = "class")
predictions
dim(test_data$M)
knn_confusion <- table(predicted, test_data$M)
# Split the data into training and testing sets
train_idx <- sample(nrow(normalized_data), nrow(normalized_data)*0.7)
train <- normalized_data[train_idx, ]
test <- normalized_data[-train_idx, ]
# Create the k-Nearest Neighbors model
k <- 5  # set the number of neighbors to consider
predicted <- knn(train[, c("V", "H", "S")], test[, c("V", "H", "S")], train$M, k)
summary(predicted)
# Evaluate the model's accuracy
actual <- test$M
accuracy <- mean(predicted == actual)
cat("Accuracy:", round(accuracy, 2))
knn_confusion <- table(predicted, test_data$M)
knn_confusion <- table(predicted, actual)
knn_confusion
knn_confusion <- table(predicted, actual)
knn_confusion
knn_confusion_df <- as.data.frame.matrix(knn_confusion)
knn_confusion_df$predicted <- rownames(knn_confusion_df)
knn_confusion_df <- tidyr::gather(knn_confusion_df, actual, value, -predicted)
# Create confusion matrix plot
ggplot(knn_confusion_df, aes(x = actual, y = predicted, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "white", high = "blue") +
theme_minimal() +
labs(x = "Actual", y = "Predicted", fill = "Count")
# Fit a multinomial logistic regression model
model <- multinom(M ~ V + H + S, data = normalized_data)
summary(model)
# Extract the test set from my_data using the test logical vector
test_data <- normalized_data[-train_idx, ]
# Make predictions on the test set
predictions <- predict(model, newdata = test_data, type = "class")
# Evaluate the performance of the model
accuracy <- mean(predictions == test_data$M)
accuracy
mlr_confusion <- table(predictions, test_data$M)
mlr_confusion
mlr_confusion <- table(predictions, test_data$M)
mlr_confusion
mlr_confusion_df <- as.data.frame.matrix(knn_confusion)
mlr_confusion_df$predicted <- rownames(knn_confusion_df)
mlr_confusion <- table(predictions, test_data$M)
mlr_confusion
mlr_confusion_df <- as.data.frame.matrix(mlr_confusion)
mlr_confusion_df$predicted <- rownames(mlr_confusion_df)
mlr_confusion_df <- tidyr::gather(mlr_confusion_df, actual, value, -predicted)
# Create confusion matrix plot
ggplot(mlr_confusion_df, aes(x = actual, y = predicted, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "white", high = "blue") +
theme_minimal() +
labs(x = "Actual", y = "Predicted", fill = "Count")
knn_confusion <- table(predicted, actual)
knn_confusion
knn_confusion_df <- as.data.frame.matrix(knn_confusion)
knn_confusion_df$predicted <- rownames(knn_confusion_df)
knn_confusion_df <- tidyr::gather(knn_confusion_df, actual, value, -predicted)
# Create confusion matrix plot
ggplot(knn_confusion_df, aes(x = actual, y = predicted, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "red", high = "blue") +
theme_minimal() +
labs(x = "Actual", y = "Predicted", fill = "Count")
knn_confusion <- table(predicted, actual)
knn_confusion
knn_confusion_df <- as.data.frame.matrix(knn_confusion)
knn_confusion_df$predicted <- rownames(knn_confusion_df)
knn_confusion_df <- tidyr::gather(knn_confusion_df, actual, value, -predicted)
# Create confusion matrix plot
ggplot(knn_confusion_df, aes(x = actual, y = predicted, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "green", high = "blue") +
theme_minimal() +
labs(x = "Actual", y = "Predicted", fill = "Count")
knn_confusion <- table(predicted, actual)
knn_confusion
knn_confusion_df <- as.data.frame.matrix(knn_confusion)
knn_confusion_df$predicted <- rownames(knn_confusion_df)
knn_confusion_df <- tidyr::gather(knn_confusion_df, actual, value, -predicted)
# Create confusion matrix plot
ggplot(knn_confusion_df, aes(x = actual, y = predicted, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "grey", high = "blue") +
theme_minimal() +
labs(x = "Actual", y = "Predicted", fill = "Count")
mlr_confusion <- table(predictions, test_data$M)
mlr_confusion
mlr_confusion_df <- as.data.frame.matrix(mlr_confusion)
mlr_confusion_df$predicted <- rownames(mlr_confusion_df)
mlr_confusion_df <- tidyr::gather(mlr_confusion_df, actual, value, -predicted)
# Create confusion matrix plot
ggplot(mlr_confusion_df, aes(x = actual, y = predicted, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "grey", high = "blue") +
theme_minimal() +
labs(x = "Actual", y = "Predicted", fill = "Count")
# Split the data into training and testing sets
train_indices <- sample(nrow(normalized_data), 0.7 * nrow(normalized_data))
train_data <- normalized_data[train_indices, ]
test_data <- normalized_data[-train_indices, ]
# Create the Naive Bayes classifier
nb_classifier <- naiveBayes(M ~ V + H + S, data = train_data)
summary(nb_classifier)
# Make predictions on the testing set
predictions <- predict(nb_classifier, test_data)
table(predictions, test_data$M)
# Calculate the accuracy of the classifier
accuracy <- sum(predictions == test_data$M) / length(predictions)
accuracy
nb_confusion <- table(predictions, test_data$M)
nb_confusion
nb_confusion_df <- as.data.frame.matrix(nb_confusion)
nb_confusion_df$predicted <- rownames(nb_confusion_df)
nb_confusion_df <- tidyr::gather(nb_confusion_df, actual, value, -predicted)
# Create confusion matrix plot
ggplot(nb_confusion_df, aes(x = actual, y = predicted, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "grey", high = "blue") +
theme_minimal() +
labs(x = "Actual", y = "Predicted", fill = "Count")
# Create a new dataset to predict on
new_data <- data.frame(V = c(1, 2, 3),
H = c(4, 5, 6),
S = c(7, 8, 9))
# Make predictions using the decision tree
predictions <- predict(tree, new_data, type = "class")
predictions
install.packages("pROC")
install.packages("pROC")
# Split the data into training and testing sets
train_indices <- sample(nrow(normalized_data), 0.7 * nrow(normalized_data))
train_data <- normalized_data[train_indices, ]
test_data <- normalized_data[-train_indices, ]
# Create the Naive Bayes classifier
nb_classifier <- naiveBayes(M ~ V + H + S, data = train_data)
nb_confusion <- table(predictions, test_data$M)
# Split the data into training and testing sets
train_indices <- sample(nrow(normalized_data), 0.7 * nrow(normalized_data))
train_data <- normalized_data[train_indices, ]
test_data <- normalized_data[-train_indices, ]
# Create the Naive Bayes classifier
nb_classifier <- naiveBayes(M ~ V + H + S, data = train_data)
library(caret)
library(class)
library(readxl)
library(rpart)
library(stats)
library(e1071)
library(nnet)
library(ggplot2)
library(gridExtra)
library(reshape2)
# Split the data into training and testing sets
train_indices <- sample(nrow(normalized_data), 0.7 * nrow(normalized_data))
train_data <- normalized_data[train_indices, ]
test_data <- normalized_data[-train_indices, ]
# Create the Naive Bayes classifier
nb_classifier <- naiveBayes(M ~ V + H + S, data = train_data)
summary(nb_classifier)
# Make predictions on the testing set
predictions <- predict(nb_classifier, test_data)
table(predictions, test_data$M)
# Calculate the accuracy of the classifier
accuracy <- sum(predictions == test_data$M) / length(predictions)
accuracy
nb_confusion <- table(predictions, test_data$M)
nb_confusion
nb_confusion_df <- as.data.frame.matrix(nb_confusion)
nb_confusion_df$predicted <- rownames(nb_confusion_df)
nb_confusion_df <- tidyr::gather(nb_confusion_df, actual, value, -predicted)
# Create confusion matrix plot
ggplot(nb_confusion_df, aes(x = actual, y = predicted, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "grey", high = "blue") +
theme_minimal() +
labs(x = "Actual", y = "Predicted", fill = "Count")
library(pROC)
predicted_probabilities <- predictions[, "M"]
library(pROC)
predicted_probabilities <- predictions$M
set.seed(123)
train_pct <- 0.7
train_size <- round(nrow(normalized_data) * train_pct)
train_indices <- sample(seq_len(nrow(normalized_data)), size = train_size)
train_data <- normalized_data[train_indices, ]
test_data <- normalized_data[-train_indices, ]
# Perform hierarchical clustering on the first three variables of the training data
dist_mat <- dist(train_data[, 1:3])
hclust_result <- hclust(dist_mat, method = "ward.D2")
# Plot the dendrogram
plot(hclust_result)
# Cut the dendrogram into three clusters and extract the cluster labels for the training data
train_clusters <- cutree(hclust_result, k = 3)
# Use the cluster labels to predict the clusters of the test data
test_clusters <- apply(test_data[, 1:3], 1, function(x) {
# Calculate the distance from each point to each cluster center
cluster_centers <- aggregate(train_data[, 1:3], list(train_clusters), mean)$x
distances <- apply(cluster_centers, 1, function(c) sqrt(sum((x - c)^2)))
# Return the index of the closest cluster
which.min(distances)
})
set.seed(123)
train_pct <- 0.7
train_size <- round(nrow(normalized_data) * train_pct)
train_indices <- sample(seq_len(nrow(normalized_data)), size = train_size)
train_data <- normalized_data[train_indices, ]
test_data <- normalized_data[-train_indices, ]
# Perform hierarchical clustering on the first three variables of the training data
dist_mat <- dist(train_data[, 1:3])
hclust_result <- hclust(dist_mat, method = "ward.D2")
# Plot the dendrogram
plot(hclust_result)
# Cut the dendrogram into three clusters and extract the cluster labels for the training data
train_clusters <- cutree(hclust_result, k = 3)
# Use the cluster labels to predict the clusters of the test data
test_clusters <- apply(test_data[, 1:3], 1, function(x) {
# Calculate the distance from each point to each cluster center
cluster_centers <- aggregate(train_data[, 1:3], list(train_clusters), mean)$x
distances <- apply(cluster_centers, 1, function(c) sqrt(sum((x - c)^2)))
# Return the index of the closest cluster
which.min(distances)
})
set.seed(123)
train_pct <- 0.7
train_size <- round(nrow(normalized_data) * train_pct)
train_indices <- sample(seq_len(nrow(normalized_data)), size = train_size)
train_data <- normalized_data[train_indices, ]
test_data <- normalized_data[-train_indices, ]
# Perform hierarchical clustering on the first three variables of the training data
dist_mat <- dist(train_data[, 1:3])
hclust_result <- hclust(dist_mat, method = "ward.D2")
# Plot the dendrogram
plot(hclust_result)
set.seed(123)
train_pct <- 0.7
train_size <- round(nrow(normalized_data) * train_pct)
train_indices <- sample(seq_len(nrow(normalized_data)), size = train_size)
train_data <- normalized_data[train_indices, ]
test_data <- normalized_data[-train_indices, ]
# Perform hierarchical clustering on the first three variables of the training data
dist_mat <- dist(train_data[, 1:3])
hclust_result <- hclust(dist_mat, method = "ward.D2")
# Perform hierarchical clustering on the first three variables of the training data
dist_mat <- dist(train_data[, 1:3])
hclust_result <- hclust(dist_mat, method = "ward.D2")
summary(hclust_result)
# Perform hierarchical clustering on the first three variables of the training data
dist_mat <- dist(train_data[, 1:3])
hclust_result <- hclust(dist_mat, centers = 3, method = "ward.D2")
# Perform hierarchical clustering on the first three variables of the training data
dist_mat <- dist(train_data[, 1:3])
hclust_result <- hclust(dist_mat, method = "ward.D2")
summary(hclust_result)
# Perform hierarchical clustering on the first three variables of the training data
dist_mat <- dist(train_data[, 1:3])
hclust_result <- hclust(dist_mat, method = "ward.D2")
summary(hclust_result)
# Plot the dendrogram
plot(hclust_result)
kmeans_result <- kmeans(train_data[, 1:3], centers = 3)
summary(kmeans_result)
plot(kmeans_result)
# Cut the dendrogram into three clusters and extract the cluster labels for the training data
train_clusters <- cutree(hclust_result, k = 3)
# Use the cluster labels to predict the clusters of the test data
test_clusters <- apply(test_data[, 1:3], 1, function(x) {
# Calculate the distance from each point to each cluster center
cluster_centers <- aggregate(train_data[, 1:3], list(train_clusters), mean)$x
distances <- apply(cluster_centers, 1, function(c) sqrt(sum((x - c)^2)))
# Return the index of the closest cluster
which.min(distances)
})
train_clusters <- cutree(hclust_result, k = 3)
# Use the cluster labels to predict the clusters of the test data
test_clusters <- apply(test_data[, 1:3], 1, function(x) {
# Calculate the distance from each point to each cluster center
cluster_centers <- aggregate(train_data[, 1:3], list(train_clusters), mean)
distances <- apply(cluster_centers[, -1], 1, function(c) sqrt(sum((x - c)^2)))
# Return the index of the closest cluster
which.min(distances)
})
print(test_clusters)
# Plot the test data with different colors representing the predicted clusters
plot(test_data[,1], test_data[,2], col = test_clusters)
# Plot the test data with different colors representing the predicted clusters
plot(test_data[,1], test_data[,2], col = test_clusters)
