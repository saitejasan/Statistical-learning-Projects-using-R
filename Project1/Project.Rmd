---
title: "Project-1"
date: "2023-04-20"
output: html_document
---

```{r}
library(caret)
library(class)
library(readxl)
library(rpart)
library(stats)
library(e1071)
library(nnet)
library(ggplot2)
library(gridExtra)
library(reshape2)
```


#### We are using land mines data for our project

```{r}
my_data <- read_excel("C:/Users/saite/OneDrive - University at Buffalo/Documents/2nd Sem Coursework/SDM2/Project1/Mine_Dataset.xls",sheet=2)
```

#### Displaying the top 20 rows of our dataset
```{r}
head(my_data, 20)
```

#### Displaying Structure of our landmines dataset including the number of rows and variables, and the type of data in each feature.
```{r}
str(my_data)
```

#### Statistics of every feature of our dataset
```{r}
summary(my_data)
```

### Normalizing the data
```{r}
# Extract the feature variables from the data
features <- my_data[, c("V", "H", "S")]

# Normalize the feature variables
normalized_features <- scale(features)

# Combine the normalized features with the target variable
normalized_data <- data.frame(normalized_features, M = my_data$M)
```


#### The dimensions of the data are obtained by dim function
```{r}
dim(normalized_data)
```

### Names of the columns present in data
```{r}
colnames(normalized_data)
```
### V - Voltage
### H - High
### S - Soil Type
### M - Mine Type

### Finding unique elements in the predictor column

```{r}
unique(normalized_data$M)
```


#### Finding the relation between Feature and target variables.

```{r}
plot(normalized_data$V, normalized_data$M, main="Voltage vs Mine Type",
   xlab="Voltage", ylab="Mine Type", pch=19)
```

```{r}
plot(normalized_data$H, normalized_data$M, main="High vs Mine Type",
   xlab="High", ylab="Mine Type", pch=19)
```

```{r}
plot(normalized_data$S, normalized_data$M, main="Soil Type vs Mine Type",
   xlab="Soil Type", ylab="Mine Type", pch=19)
```

```{r}
cor(normalized_data)
```

### Voltage type distribution
```{r}
ggplot(my_data, aes(x=normalized_data$V, y=normalized_data$M)) +
  geom_line(color="violet", size=1) +
  labs(title="My Line Graph", x="X-Axis Label", y="Y-Axis Label")
```


### Barchart to visualize different mine types
```{r}
mine_colors <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2")

ggplot(normalized_data, aes(x = factor(M), fill = factor(M))) + 
  geom_bar() +
  scale_fill_manual(values = mine_colors) +
  labs(title = "Counts of Mine Types", x = "Mine Type", y = "Count")
```

### Boxplot to find the outliers
```{r}
ggplot(melt(my_data, id.vars="M"), aes(x=variable, y=value)) +
  geom_boxplot() +
  facet_wrap(~ variable, scales="free") +
  labs(title="Box plot of all feature variables")
```


## Algorithms to be Performed
### K-Means Clusteting
```{r}
set.seed(123)
train_pct <- 0.7
train_size <- round(nrow(normalized_data) * train_pct)
train_indices <- sample(seq_len(nrow(normalized_data)), size = train_size)
train_data <- normalized_data[train_indices, ]
test_data <- normalized_data[-train_indices, ]
```

```{r}
kmeans_result <- kmeans(train_data[, 1:3], centers = 3)
summary(kmeans_result)
```

```{r}
centers <- kmeans_result$centers
```

```{r}
# Use the cluster centers to predict the clusters of the test data
test_clusters <- apply(test_data[, 1:3], 1, function(x) {
  # Calculate the distance from each point to each center
  distances <- apply(centers, 1, function(c) sqrt(sum((x - c)^2)))
  
  # Return the index of the closest center
  which.min(distances)
})
print(test_clusters)
```

```{r}
plot(test_data[,1], test_data[,2], col = test_clusters)
```


### Hirerachial Clustering
```{r}
set.seed(123)
train_pct <- 0.7
train_size <- round(nrow(normalized_data) * train_pct)
train_indices <- sample(seq_len(nrow(normalized_data)), size = train_size)
train_data <- normalized_data[train_indices, ]
test_data <- normalized_data[-train_indices, ]
```

```{r}
# Perform hierarchical clustering on the first three variables of the training data
dist_mat <- dist(train_data[, 1:3])
hclust_result <- hclust(dist_mat, method = "ward.D2")
summary(hclust_result)
# Plot the dendrogram
plot(hclust_result)
```

```{r}
train_clusters <- cutree(hclust_result, k = 3)

# Use the cluster labels to predict the clusters of the test data
test_clusters <- apply(test_data[, 1:3], 1, function(x) {
  # Calculate the distance from each point to each cluster center
  cluster_centers <- aggregate(train_data[, 1:3], list(train_clusters), mean)
  distances <- apply(cluster_centers[, -1], 1, function(c) sqrt(sum((x - c)^2)))
  
  # Return the index of the closest cluster
  which.min(distances)
})
print(test_clusters)
```

```{r}
# Plot the test data with different colors representing the predicted clusters
plot(test_data[,1], test_data[,2], col = test_clusters)
```


### Decision tree algorithm
```{r}
# Create the decision tree
tree <- rpart(M ~ V + H + S, data = normalized_data, method = "class")

# Plot the decision tree
plot(tree)
text(tree)
```

```{r}
# Create a new dataset to predict on
new_data <- data.frame(V = c(1, 2, 3),
                       H = c(4, 5, 6),
                       S = c(7, 8, 9))

# Make predictions using the decision tree
predictions <- predict(tree, new_data, type = "class")
predictions
```


### Neural Networks
```{r}
set.seed(123)
train_idx <- sample(nrow(normalized_data), nrow(normalized_data)*0.7)
train_data <- normalized_data[train_idx, ]
test_data <- normalized_data[-train_idx, ]

# Fit a neural network model
nnet_model <- nnet(M ~ V + H + S, data = train_data, size = 3)

# Make predictions on the test set
nnet_predictions <- predict(nnet_model, newdata = test_data)

# Evaluate the performance of the model
nnet_accuracy <- mean(nnet_predictions == test_data$M)
nnet_accuracy
```

#### Confusion Matrix
```{r}
nnet_confusion <- table(nnet_predictions, test_data$M)
nnet_confusion

nnet_confusion_df <- as.data.frame.matrix(nnet_confusion)
nnet_confusion_df$predicted <- rownames(nnet_confusion_df)
nnet_confusion_df <- tidyr::gather(nnet_confusion_df, actual, value, -predicted)

# Create confusion matrix plot
ggplot(nnet_confusion_df, aes(x = actual, y = predicted, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(x = "Actual", y = "Predicted", fill = "Count")
```


### KNN
```{r}
# Split the data into training and testing sets
train_idx <- sample(nrow(normalized_data), nrow(normalized_data)*0.7)
train <- normalized_data[train_idx, ]
test <- normalized_data[-train_idx, ]

# Create the k-Nearest Neighbors model
k <- 5  # set the number of neighbors to consider
predicted <- knn(train[, c("V", "H", "S")], test[, c("V", "H", "S")], train$M, k)
summary(predicted)

# Evaluate the model's accuracy
actual <- test$M
accuracy <- mean(predicted == actual)
cat("Accuracy:", round(accuracy, 2))
```

#### Confusion Matrix
```{r}
knn_confusion <- table(predicted, actual)
knn_confusion

knn_confusion_df <- as.data.frame.matrix(knn_confusion)
knn_confusion_df$predicted <- rownames(knn_confusion_df)
knn_confusion_df <- tidyr::gather(knn_confusion_df, actual, value, -predicted)

# Create confusion matrix plot
ggplot(knn_confusion_df, aes(x = actual, y = predicted, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "grey", high = "blue") +
  theme_minimal() +
  labs(x = "Actual", y = "Predicted", fill = "Count")
```


### Multinomial Logistic Regression
```{r}

# Fit a multinomial logistic regression model
model <- multinom(M ~ V + H + S, data = normalized_data)
summary(model)
# Extract the test set from my_data using the test logical vector
test_data <- normalized_data[-train_idx, ]

# Make predictions on the test set
predictions <- predict(model, newdata = test_data, type = "class")

# Evaluate the performance of the model
accuracy <- mean(predictions == test_data$M)
accuracy
```

#### Confusion Matrix
```{r}
mlr_confusion <- table(predictions, test_data$M)
mlr_confusion

mlr_confusion_df <- as.data.frame.matrix(mlr_confusion)
mlr_confusion_df$predicted <- rownames(mlr_confusion_df)
mlr_confusion_df <- tidyr::gather(mlr_confusion_df, actual, value, -predicted)

# Create confusion matrix plot
ggplot(mlr_confusion_df, aes(x = actual, y = predicted, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "grey", high = "blue") +
  theme_minimal() +
  labs(x = "Actual", y = "Predicted", fill = "Count")
```


### Naive Bayes
```{r}
  
  # Split the data into training and testing sets
  train_indices <- sample(nrow(normalized_data), 0.7 * nrow(normalized_data))
  train_data <- normalized_data[train_indices, ]
  test_data <- normalized_data[-train_indices, ]
  
  # Create the Naive Bayes classifier
  nb_classifier <- naiveBayes(M ~ V + H + S, data = train_data)
  summary(nb_classifier)
  # Make predictions on the testing set
  predictions <- predict(nb_classifier, test_data)
  table(predictions, test_data$M)
  
  # Calculate the accuracy of the classifier
  accuracy <- sum(predictions == test_data$M) / length(predictions)
  accuracy
```

#### Confusion Matrix
```{r}
nb_confusion <- table(predictions, test_data$M)
nb_confusion

nb_confusion_df <- as.data.frame.matrix(nb_confusion)
nb_confusion_df$predicted <- rownames(nb_confusion_df)
nb_confusion_df <- tidyr::gather(nb_confusion_df, actual, value, -predicted)

# Create confusion matrix plot
ggplot(nb_confusion_df, aes(x = actual, y = predicted, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "grey", high = "blue") +
  theme_minimal() +
  labs(x = "Actual", y = "Predicted", fill = "Count")
```